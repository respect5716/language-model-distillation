defaults:
  - _self_
  - lang:
    - en
  - size:
    - s2

debug: true

working_dir: ${hydra:runtime.cwd}
save_dir: ${working_dir}/transformers
data_dir: ${working_dir}/data
model_name_or_path: ${lang.model_name_or_path}


data:
  batch_size: 8


teacher:
  pretrained_model_name_or_path: ${model_name_or_path}
  hidden_dropout_prob: 0
  attention_probs_dropout_prob: 0
  output_attentions: true
  output_hidden_states: true


student:
  pretrained_model_name_or_path: ${model_name_or_path}
  num_hidden_layers: ${size.num_hidden_layers}
  hidden_size: ${size.hidden_size}
  intermediate_size: ${size.intermediate_size}
  num_attention_heads: ${size.num_attention_heads}
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  output_attentions: true
  output_hidden_states: true


optimizer:
  name: adamw
  lr: 6e-4
  betas:
    - 0.9
    - 0.98
  weight_decay: 0.01


scheduler:
  name: linear
  num_train_steps: ${train.num_train_steps}
  warmup_ratio: 0.01


train:
  num_train_steps: 100000


lite:
  gpus: -1
  strategy: ddp
  precision: 32