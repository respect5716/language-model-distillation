
working_dir: ${hydra:runtime.cwd}
save_dir: ${working_dir}/transformers
data_dir: ${working_dir}/data
model_name_or_path: klue/bert-base


data:
  batch_size: 8
  lang: ko
  files:
    ko:
      - namuwiki
      - modu-spoken
      - modu-written
      - modu-newspaper
    en:
      - bookcorpus
      - wikitext


teacher:
  pretrained_model_name_or_path: ${model_name_or_path}
  hidden_dropout_prob: 0
  attention_probs_dropout_prob: 0
  output_attentions: true
  output_hidden_states: true


student:
  pretrained_model_name_or_path: ${model_name_or_path}
  num_hidden_layers: 6
  hidden_size: 384
  intermediate_size: 1536
  num_attention_heads: 12
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  output_attentions: true
  output_hidden_states: true


optimizer:
  name: adamw
  lr: 6e-4
  betas:
    - 0.9
    - 0.98
  weight_decay: 0.01


scheduler:
  name: linear
  num_train_steps: ${train.num_train_steps}
  warmup_ratio: 0.05


train:
  num_train_steps: 100000
  num_clusters: 3000
  niter: 30
  nredo: 1
  spherical: true
  db_size: 100000
  db_path: db.npy
  temperature: 1.
  global_alpha: 5.
  local_alpha: 100.
  local_k: 100
  num_local_heads: 24


lite:
  gpus: -1
  strategy: deepspeed
  precision: 32