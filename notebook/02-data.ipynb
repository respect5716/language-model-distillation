{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f7238a",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27828d3b",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a02bd640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "bad186d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "overrides = ['working_dir=/workspace/language-model-distillation']\n",
    "with hydra.initialize(config_path='../configs'):\n",
    "    config = hydra.compose('config', overrides=overrides, return_hydra_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e997de83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs/2021-11-25/04-58-45'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.hydra.run.dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ecf43d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.teacher, self.student = self.prepare_models()\n",
    "    \n",
    "    def prepare_models(self):\n",
    "        teacher = AutoModel.from_pretrained(\n",
    "            self.hparams.teacher.name_or_model_path,\n",
    "            output_attentions = True,\n",
    "            output_hidden_states = True\n",
    "        )\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(\n",
    "            self.hparams.teacher.name_or_model_path,\n",
    "            output_attention = True,\n",
    "            output_hidden_states = True,\n",
    "            **self.hparams.student\n",
    "        )\n",
    "        \n",
    "        student = AutoModel.from_config(config)\n",
    "        \n",
    "        for param in teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        return teacher, student\n",
    "    \n",
    "    \n",
    "    def student_param_groups(self):\n",
    "        no_decay = [\"bias\", \"bn\", \"ln\", \"norm\"]\n",
    "        param_groups = [\n",
    "            {\n",
    "                # apply weight decay\n",
    "                \"params\": [p for n, p in self.student.named_parameters() if not any(nd in n.lower() for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.optim.weight_decay\n",
    "            },\n",
    "            {\n",
    "                # not apply weight decay\n",
    "                \"params\": [p for n, p in self.student.named_parameters() if any(nd in n.lower() for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        return param_groups\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.student_param_groups(), \n",
    "            lr = self.hparams.optim.lr, \n",
    "            betas = self.hparams.optim.betas,\n",
    "            weight_decay = self.hparams.optim.weight_decay,\n",
    "            eps = self.hparams.optim.adam_epsilon,\n",
    "        )\n",
    "\n",
    "        num_training_steps = self.hparams.max_step\n",
    "        num_warmup_steps = int(num_training_steps * self.hparams.warmup_ratio)\n",
    "        scheduler = get_scheduler(self.hparams.scheduler, optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'step',\n",
    "                'frequency': self.hparams.optim.accumulate_grad_batches,\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c69797cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "m = BaseModel(**config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33018d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f5ce45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee047255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.teacher, self.student = self.prepare_models()\n",
    "        \n",
    "        \n",
    "    def prepare_models(self):\n",
    "        self.teacher = AutoModel.from_pretrained(\n",
    "            self.hparams.teacher.name_or_model_path,\n",
    "            output_attentions = True,\n",
    "            output_hidden_states = True\n",
    "        )\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(\n",
    "            self.hparams.teacher.name_or_model_path,\n",
    "            output_attention = True,\n",
    "            output_hidden_states = True,\n",
    "            **self.hparams.student\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, batch, phase):\n",
    "        input_ids, labels = batch\n",
    "        attention_mask = input_ids.ne(self.teacher.config.pad_token_id).float()\n",
    "        \n",
    "        teacher_outputs = self.teacher(input_ids, attention_mask=attention_mask)\n",
    "        student_outputs = self.student(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        tk, tq, tv = self.teacher.q[self.hparams.teacher_layer_index], self.teacher.k[self.hparams.teacher_layer_index], self.teacher.v[self.hparams.teacher_layer_index] # (batch, head, seq, head_dim)\n",
    "        sk, sq, sv = self.student.q[self.hparams.student_layer_index], self.student.k[self.hparams.student_layer_index], self.student.v[self.hparams.student_layer_index] # (batch, head, seq, head_dim)\n",
    "\n",
    "        loss_q = minilm_loss(tq, sq, self.hparams.num_relation_heads, attention_mask=attention_mask)\n",
    "        loss_k = minilm_loss(tk, sk, self.hparams.num_relation_heads, attention_mask=attention_mask)\n",
    "        loss_v = minilm_loss(tv, sv, self.hparams.num_relation_heads, attention_mask=attention_mask)\n",
    "        loss = loss_q + loss_k + loss_v\n",
    "\n",
    "        self.log_dict({f'{phase}/loss': loss}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, 'valid')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
