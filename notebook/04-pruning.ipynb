{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae1a15b",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7543e",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f3c235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  8 04:29:29 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  TITAN RTX           On   | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| 66%   84C    P2   248W / 280W |   8738MiB / 24219MiB |     93%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c259698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import hydra\n",
    "import GPUtil \n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from transformers import AutoModel, AutoModelForMaskedLM, AutoTokenizer, AutoConfig\n",
    "from transformers import get_scheduler\n",
    "from transformers import BatchEncoding\n",
    "from transformers.data.data_collator import DataCollatorForWholeWordMask\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf679402",
   "metadata": {},
   "outputs": [],
   "source": [
    "with hydra.initialize('../configs'):\n",
    "    config = hydra.compose('config.yaml', overrides=['working_dir=../', 'model.mlm=false', 'data.batch_size=16'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d2bd9",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b6f9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer, batch_size=8):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.dataset = load_dataset('text', data_files=os.path.join('../data', 'kowiki.txt'))['train']\n",
    "        self.dataset.set_transform(lambda batch: transform(batch, self.tokenizer, 512))\n",
    "        self.dataset = self.dataset.train_test_split(test_size=0.01)\n",
    "        self.train_dataset, self.eval_dataset = self.dataset['train'], self.dataset['test']\n",
    "        \n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        batch = BatchEncoding(batch)\n",
    "        batch['attention_mask'] = batch.input_ids.ne(self.tokenizer.pad_token_id).float()\n",
    "        return batch\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def validation_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.eval_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "def transform(batch, tokenizer, max_length):\n",
    "    new_batch = []\n",
    "    for text in batch['text']:\n",
    "        text = slice_text(text)\n",
    "        new_batch.append(text)\n",
    "    \n",
    "    return tokenizer(new_batch, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "\n",
    "def slice_text(text, max_char_length=1024):\n",
    "    if len(text) > max_char_length:\n",
    "        idx = np.random.randint(low=0, high=len(text)-max_char_length)\n",
    "        text = text[idx : idx+max_char_length]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf67acf1",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6084ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_layers(model: torch.nn.Module, indices: List[int]):\n",
    "    model.encoder.layer = nn.ModuleList([l for i, l in enumerate(model.encoder.layer) if i in indices])\n",
    "    return model\n",
    "\n",
    "def select_indices_from_embedding(embedding: torch.Tensor, num_features: int = 384, corr_threshold: float = 0.3):\n",
    "    corr = torch.corrcoef(embedding.transpose(-1, -2))\n",
    "    corr = corr - torch.eye(corr.size(0))\n",
    "    \n",
    "    removed_indices = []\n",
    "    for idx in range(corr.size(0)):\n",
    "        most_sim_idx = corr[idx].argmax()\n",
    "        if corr[idx][most_sim_idx] > corr_threshold and idx > most_sim_idx:\n",
    "            removed_indices.append(idx)\n",
    "            \n",
    "    norm = embedding.norm(dim=0)\n",
    "    norm_indices = (-norm).argsort()\n",
    "\n",
    "    indices = [int(i) for i in norm_indices if i not in removed_indices]\n",
    "    indices = indices[:num_features]\n",
    "    return indices\n",
    "\n",
    "def select_weight(weight, indices, dim):\n",
    "    if type(dim) == int:\n",
    "        indices = torch.tensor(indices)\n",
    "        return torch.index_select(weight, dim, indices.to(weight.device))\n",
    "    \n",
    "    else:\n",
    "        for d in dim:\n",
    "            weight = select_weight(weight, indices, d)\n",
    "        return weight\n",
    "    \n",
    "AttributeError: 'Embedding' object has no attribute 'clone'\n",
    "\n",
    "\n",
    "def select_layernorm(layernorm, indices):\n",
    "    layernorm.weight.data = select_weight(layernorm.weight.data, indices, dim=0)\n",
    "    layernorm.bias.data = select_weight(layernorm.bias.data, indices, dim=0)\n",
    "    layernorm.normalized_shape = (len(indices), )\n",
    "    return layernorm\n",
    "\n",
    "def select_linear(linear, indices, dims=[0, 1]):\n",
    "    linear.weight.data = select_weight(linear.weight.data, indices, dim=dims)\n",
    "    if 0 in dims:\n",
    "        linear.bias.data = select_weight(linear.bias.data, indices, dim=0)\n",
    "        \n",
    "    if 1 in dims:\n",
    "        linear.in_features = len(indices)\n",
    "    if 0 in dims:\n",
    "        linear.out_features = len(indices)\n",
    "    return linear\n",
    "\n",
    "def select_bert_embeddings(bert_embeddings, indices):\n",
    "    bert_embeddings.word_embeddings = select_embedding(bert_embeddings.word_embeddings, indices)\n",
    "    bert_embeddings.position_embeddings = select_embedding(bert_embeddings.position_embeddings, indices)\n",
    "    bert_embeddings.token_type_embeddings = select_embedding(bert_embeddings.token_type_embeddings, indices)\n",
    "    bert_embeddings.LayerNorm = select_layernorm(bert_embeddings.LayerNorm, indices)\n",
    "    return bert_embeddings\n",
    "\n",
    "\n",
    "def select_bert_layer(bert_layer, indices):\n",
    "    bert_layer.attention.self.all_head_size = len(indices)\n",
    "    bert_layer.attention.self.attention_head_size = len(indices) // bert_layer.attention.self.num_attention_heads\n",
    "    \n",
    "    bert_layer.attention.self.query = select_linear(bert_layer.attention.self.query, indices)\n",
    "    bert_layer.attention.self.key = select_linear(bert_layer.attention.self.key, indices)\n",
    "    bert_layer.attention.self.value = select_linear(bert_layer.attention.self.value, indices)\n",
    "    \n",
    "    bert_layer.attention.output.dense = select_linear(bert_layer.attention.output.dense, indices)\n",
    "    bert_layer.attention.output.LayerNorm = select_layernorm(bert_layer.attention.output.LayerNorm, indices)\n",
    "    \n",
    "    bert_layer.intermediate.dense = select_linear(bert_layer.intermediate.dense, indices, dims=[1])\n",
    "    bert_layer.output.dense = select_linear(bert_layer.output.dense, indices, dims=[0])\n",
    "    bert_layer.output.LayerNorm = select_layernorm(bert_layer.output.LayerNorm, indices)\n",
    "    return bert_layer\n",
    "\n",
    "def select_bert_pooler(bert_pooler, indices):\n",
    "    bert_pooler.dense = select_linear(bert_pooler.dense, indices)\n",
    "    return bert_pooler\n",
    "\n",
    "def select_bert_model(bert_model, layer_indices, weight_indices):\n",
    "    bert_model = select_layers(bert_model, layer_indices)\n",
    "    bert_model.embeddings = select_bert_embeddings(bert_model.embeddings, weight_indices)\n",
    "    bert_model.encoder.layer = nn.ModuleList([select_bert_layer(l, weight_indices) for l in bert_model.encoder.layer])\n",
    "    bert_model.pooler = select_bert_pooler(bert_model.pooler, weight_indices)\n",
    "    return bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e3ac1800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_distill(model):\n",
    "    model.base_model.encoder.layer[0].attention.self.__class__._forward = bert_self_attention_forward\n",
    "    for layer in model.base_model.encoder.layer:\n",
    "        layer.attention.self.forward = layer.attention.self._forward\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "\n",
    "def bert_self_attention_forward(\n",
    "    self,\n",
    "    hidden_states,\n",
    "    attention_mask=None,\n",
    "    head_mask=None,\n",
    "    encoder_hidden_states=None,\n",
    "    encoder_attention_mask=None,\n",
    "    past_key_value=None,\n",
    "    output_attentions=False,\n",
    "):\n",
    "    mixed_query_layer = self.query(hidden_states)\n",
    "    mixed_key_layer = self.key(hidden_states)\n",
    "    mixed_value_layer = self.value(hidden_states)\n",
    "    \n",
    "    query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "    key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "    value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "    \n",
    "    self.q = mixed_query_layer # (Batch, Seq, Dim)\n",
    "    self.k = mixed_key_layer # (Batch, Seq, Dim)\n",
    "    self.v = mixed_value_layer # (Batch, Seq, Dim)\n",
    "\n",
    "    if self.is_decoder:\n",
    "        past_key_value = (key_layer, value_layer)\n",
    "\n",
    "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "    if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "        seq_length = hidden_states.size()[1]\n",
    "        position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "        position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "        distance = position_ids_l - position_ids_r\n",
    "        positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "        positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\":\n",
    "            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "            attention_scores = attention_scores + relative_position_scores\n",
    "        elif self.position_embedding_type == \"relative_key_query\":\n",
    "            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "            attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "    if attention_mask is not None:\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "    attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "    attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "    if head_mask is not None:\n",
    "        attention_probs = attention_probs * head_mask\n",
    "\n",
    "    context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "    new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "    context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "    outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "    if self.is_decoder:\n",
    "        outputs = outputs + (past_key_value,)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def get_qkvs(model):\n",
    "    attns = [l.attention.self for l in model.base_model.encoder.layer]\n",
    "    qkvs = [{'q': a.q, 'k': a.k, 'v': a.v} for a in attns]    \n",
    "    return qkvs\n",
    "\n",
    "def transpose_for_scores(h, num_heads):\n",
    "    batch_size, seq_length, dim = h.size()\n",
    "    head_size = dim // num_heads\n",
    "    h = h.view(batch_size, seq_length, num_heads, head_size)\n",
    "    return h.permute(0, 2, 1, 3) # (batch, num_heads, seq_length, head_size)\n",
    "\n",
    "\n",
    "def attention(h1, h2, num_heads, attention_mask=None):\n",
    "    assert h1.size() == h2.size()\n",
    "    head_size = h1.size(-1) // num_heads\n",
    "    h1 = transpose_for_scores(h1, num_heads) # (batch, num_heads, seq_length, head_size)\n",
    "    h2 = transpose_for_scores(h2, num_heads) # (batch, num_heads, seq_length, head_size)\n",
    "\n",
    "    attn = torch.matmul(h1, h2.transpose(-1, -2)) # (batch_size, num_heads, seq_length, seq_length)\n",
    "    attn = attn / math.sqrt(head_size)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask[:, None, None, :]\n",
    "        attention_mask = (1 - attention_mask) * -10000.0\n",
    "        attn = attn + attention_mask\n",
    "\n",
    "    return attn\n",
    "\n",
    "\n",
    "def kl_div_loss(s, t, temperature=1.):\n",
    "    if len(s.size()) != 2:\n",
    "        s = s.view(-1, s.size(-1))\n",
    "        t = t.view(-1, t.size(-1))\n",
    "\n",
    "    s = F.log_softmax(s / temperature, dim=-1)\n",
    "    t = F.softmax(t / temperature, dim=-1)\n",
    "    return F.kl_div(s, t, reduction='batchmean')\n",
    "\n",
    "def minilm_loss(t, s, num_relation_heads, attention_mask=None):\n",
    "    attn_t = attention(t, t, num_relation_heads, attention_mask)\n",
    "    attn_s = attention(s, s, num_relation_heads, attention_mask)\n",
    "    loss = kl_div_loss(attn_s, attn_t)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb701bc",
   "metadata": {},
   "source": [
    "## 4. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "610b6102",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb9355f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-82324f4e586d6530\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-82324f4e586d6530/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e646c9cf0f5f44c0a0c4ae2e491ee201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_module = DataModule(tokenizer)\n",
    "data_module.setup()\n",
    "loader = iter(data_module.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a661444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "511e1bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('klue/bert-base', output_hidden_states=True, output_attentions=True)\n",
    "model = to_distill(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "6a4d08f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_indices = select_indices_from_embedding(model.embeddings.word_embeddings.weight.data, num_features=384, corr_threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e3841bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned = select_bert_model(model, layer_indices=list(range(6)), weight_indices=weight_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3f884061",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pruned(**batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec863e",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27c03bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_scores = np.zeros(12)\n",
    "weight_scores = np.zeros(768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6b04abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b894e3af5179411687b2f853b878dc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6470545ff324c71ab61d25157c0649f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 18% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 26% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 31% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 36% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 86% | 21% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 25% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 31% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 37% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 41% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 46% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 51% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 56% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 82% | 22% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 27% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 90% | 32% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 38% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 42% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 47% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 53% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 58% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 24% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 29% |\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 34% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 89% | 40% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 44% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 49% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 55% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 59% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 90% | 26% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 31% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 35% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 41% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 46% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 94% | 51% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 56% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 85% | 21% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 28% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 32% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 37% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 43% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 47% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 52% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 58% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 90% | 23% |\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 29% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 34% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 39% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 44% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 49% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 54% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 59% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 25% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 31% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 35% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 40% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 46% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 50% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 55% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 83% | 22% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 26% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 32% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 36% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 41% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 47% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 51% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 94% | 57% |\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 90% | 22% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 28% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 33% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 39% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 94% | 44% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 49% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 54% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 58% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 24% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 29% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 35% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 39% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 94% | 45% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 51% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 55% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 22% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 90% | 26% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 31% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 37% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 42% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 94% | 47% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 52% |\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 57% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 85% | 22% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 27% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 32% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 39% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 43% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 48% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 53% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 91% | 57% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 25% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 29% |\n",
      "None\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 93% | 34% |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(range(1000))\n",
    "for i in tqdm(pbar):\n",
    "    layer_indices = np.random.choice(range(12), size=6, replace=False).tolist()\n",
    "    weight_indices = np.random.choice(range(768), size=384, replace=False).tolist()\n",
    "    \n",
    "    batch = next(loader)\n",
    "    batch = {k:v.cuda() for k,v in batch.items()}\n",
    "    \n",
    "    pruned = copy.deepcopy(model)\n",
    "    pruned = select_bert_model(pruned, layer_indices, weight_indices)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_out = model(**batch)\n",
    "        pruned_out = pruned(**batch)\n",
    "    \n",
    "    model_qkvs = get_qkvs(model)[-1]\n",
    "    pruned_qkvs = get_qkvs(pruned)[-1]\n",
    "    \n",
    "    loss_q = minilm_loss(model_qkvs['q'], pruned_qkvs['q'], 24, batch['attention_mask'])\n",
    "    loss_k = minilm_loss(model_qkvs['k'], pruned_qkvs['k'], 24, batch['attention_mask'])\n",
    "    loss_v = minilm_loss(model_qkvs['v'], pruned_qkvs['v'], 24, batch['attention_mask'])\n",
    "    loss = loss_q + loss_k + loss_v\n",
    "    loss = float(loss)\n",
    "    \n",
    "    layer_scores[layer_indices] += loss\n",
    "    weight_scores[weight_indices] += loss\n",
    "    \n",
    "    del pruned\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    memory = torch.cuda.memory_allocated() / 2**20\n",
    "    pbar.set_postfix({'memory': round(memory, 3)})\n",
    "    \n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(GPUtil.showUtilization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14f15f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(x=range(12), height=layer_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bf0c9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x=range(768), y=weight_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae4760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_indices = np.argsort(layer_scores)[:6].tolist()\n",
    "weight_indices = np.argsort(weight_scores)[:384].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned = copy.deepcopy(model)\n",
    "pruned = select_bert_model(pruned, layer_indices, weight_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "2004ed02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../init/transformers/tokenizer_config.json',\n",
       " '../init/transformers/special_tokens_map.json',\n",
       " '../init/transformers/vocab.txt',\n",
       " '../init/transformers/added_tokens.json',\n",
       " '../init/transformers/tokenizer.json')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned.config.hidden_size = 384\n",
    "pruned.config.num_hidden_layers = 6\n",
    "\n",
    "pruned.save_pretrained('../init/transformers')\n",
    "tokenizer.save_pretrained('../init/transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f64fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
